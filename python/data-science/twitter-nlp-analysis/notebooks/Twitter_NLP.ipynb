{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 1. Data Sources**"
      ],
      "metadata": {
        "id": "AXOJrmxq7Z8R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "Xgk39RLCZY_w",
        "outputId": "536ed885-839c-483b-d66d-9ea9536393d8",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "Dataset not found at ./Train.csv. Ensure you extracted Tweets.csv.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-66f91a6961c7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Dataset not found at {csv_path}. Ensure you extracted Tweets.csv.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# Read the dataset into a Pandas DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Dataset not found at ./Train.csv. Ensure you extracted Tweets.csv."
          ]
        }
      ],
      "source": [
        "# ----------------------------------------------\n",
        "# Step 1: Data Sources\n",
        "# ----------------------------------------------\n",
        "# This script performs the following:\n",
        "# 1. Loads a manually downloaded Twitter dataset from Kaggle\n",
        "# 2. Loads the dataset into a Pandas DataFrame\n",
        "# 3. Extracts a subset of 4,000 tweets for analysis\n",
        "# ----------------------------------------------\n",
        "\n",
        "# ---- 1. Environment Setup ----\n",
        "# Install necessary libraries\n",
        "\n",
        "# If 'pandas' is not found, it installs it using 'pip' and then imports it.\n",
        "# Pandas is essential for handling and analyzing structured data in tabular form,\n",
        "# making it a key tool for data preprocessing, filtering, and manipulation.\n",
        "\n",
        "try:\n",
        "    import pandas as pd\n",
        "except ImportError: # If any module is missing, handle the error\n",
        "    import os\n",
        "    os.system('pip install pandas')\n",
        "    import pandas as pd\n",
        "\n",
        "# ---- 2. Load the Dataset ----\n",
        "import os\n",
        "\n",
        "# Ensure the dataset exists\n",
        "csv_path = \"./Train.csv\"  # Update this path if the file is located elsewhere\n",
        "\n",
        "if not os.path.exists(csv_path):\n",
        "    raise FileNotFoundError(f\"Dataset not found at {csv_path}. Ensure you extracted Tweets.csv.\")\n",
        "\n",
        "# Read the dataset into a Pandas DataFrame\n",
        "df = pd.read_csv(csv_path, encoding=\"ISO-8859-1\")\n",
        "\n",
        "# Display basic information about the dataset\n",
        "print(\"\\n################################################################################################\")\n",
        "print(\"Step 1: Data Sources\\n\")\n",
        "\n",
        "print(\"Dataset Loaded Successfully!\")\n",
        "print(\"Number of rows:\", df.shape[0])\n",
        "print(\"Number of columns:\", df.shape[1], \"\\n\")\n",
        "\n",
        "\n",
        "print(\"Original Tweets dataset:\")\n",
        "print(df.head())  # Show the first few rows\n",
        "print(\"\\n\")\n",
        "\n",
        "# ---- 3. Extract a Subset of 4,000 Tweets ----\n",
        "# Since the dataset is large, we take a random subset for efficient analysis computation.\n",
        "# Keeping only the 'text' (tweet content) column for this study.\n",
        "\n",
        "df_subset = df[['text', 'brand', 'emotion']].sample(n=4000, random_state=42)\n",
        "print(\"New Tweets subset (only 'text' column selected):\")\n",
        "print(df_subset.head())  # Check the first few rows\n",
        "print(\"\\n################################################################################################\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 2. Preprocessing**"
      ],
      "metadata": {
        "id": "-IqL1R4G7gaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------\n",
        "# Step 2: Preprocessing\n",
        "# ----------------------------------------------\n",
        "# This step prepares the text data for BERT-based topic modeling.\n",
        "# Since BERT is a contextual model, we will:\n",
        "# 1. Remove URLs, mentions (@user), and special characters\n",
        "# 2. Convert emojis and contractions to meaningful words\n",
        "# 3. Remove extra spaces and format text properly\n",
        "# 4. Retain capitalization (BERT understands case-sensitive context)\n",
        "# ----------------------------------------------\n",
        "\n",
        "# ---- 1. Install Necessary Libraries ----\n",
        "# Ensure required libraries are installed\n",
        "\n",
        "try:\n",
        "    import re  # Regular expressions for text cleaning (e.g., removing special characters, URLs)\n",
        "    import emoji  # To handle and remove emojis from the text if necessary\n",
        "    import contractions  # To expand shortened words (e.g., \"can't\" -> \"cannot\")\n",
        "except ImportError:  # If any module is missing, handle the error\n",
        "    os.system('pip install emoji contractions')  # Install missing libraries\n",
        "    import re, emoji, contractions  # Re-import after installation to ensure availability\n",
        "\n",
        "# ---- 2. Define Cleaning Functions ----\n",
        "#we are analysing the data set for topic modeling, keeping mentions of usernames and hashtags is useful\n",
        "# because they contain valuable context.\n",
        "\n",
        "def clean_text(text):\n",
        "    text = contractions.fix(text)  # Expand contractions\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)  # Remove URLs\n",
        "    text = emoji.demojize(text)  # Convert emojis to text\n",
        "\n",
        "    # Keep @mentions and #hashtags while removing other special characters\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s@#:_]', '', text)\n",
        "\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
        "    return text\n",
        "\n",
        "# ---- 3. Apply Preprocessing to Dataset ----\n",
        "# Cleaning all tweets in the subset\n",
        "\n",
        "df_subset['clean_text'] = df_subset['text'].apply(clean_text)\n",
        "\n",
        "# ---- 4. Display Processed Data ----\n",
        "print(\"Step 2: Preprocessing\\n\")\n",
        "\n",
        "print(\"Preprocessing Complete!\\n\")\n",
        "\n",
        "print(\"Original Tweet vs Cleaned Tweet:\")\n",
        "print(df_subset[['text', 'clean_text']].head())  # Compare before and after cleaning\n",
        "print(\"\\n################################################################################################\")"
      ],
      "metadata": {
        "id": "mXMH3Vee6u2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 3. Exploratory Data Analysis (EDA)**"
      ],
      "metadata": {
        "id": "aAlcja1P7vtM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------\n",
        "# Step 3: Exploratory Data Analysis (EDA)\n",
        "# ----------------------------------------------\n",
        "# This step explores the dataset to gain insights before topic modeling.\n",
        "# We will:\n",
        "# 1. Analyze the distribution of tweet lengths\n",
        "# 2. Generate a word cloud of the most common words\n",
        "# 3. Identify and analyze hashtags\n",
        "# 4. Identify and analyze mentions (@username)\n",
        "# 5. Check for missing values or anomalies\n",
        "# ----------------------------------------------\n",
        "\n",
        "# ---- 1. Install Necessary Libraries ----\n",
        "try:\n",
        "    import matplotlib.pyplot as plt  # For visualizations\n",
        "    import seaborn as sns  # For aesthetic statistical visualizations\n",
        "    from collections import Counter  # For word frequency analysis\n",
        "    from wordcloud import WordCloud  # For generating word clouds\n",
        "except ImportError:\n",
        "    import os\n",
        "    os.system('pip install matplotlib seaborn wordcloud')\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    from collections import Counter\n",
        "    from wordcloud import WordCloud\n",
        "\n",
        "# ---- 2. Check for Missing Values ----\n",
        "print(\"Step 3: Exploratory Data Analysis (EDA)\\n\")\n",
        "\n",
        "missing_values = df_subset.isnull().sum()\n",
        "print(\"Missing Values in Each Column:\")\n",
        "print(missing_values)\n",
        "print(\"\\n\")\n",
        "\n",
        "# ---- 3. Analyze Tweet Length Distribution ----\n",
        "df_subset['text_length'] = df_subset['clean_text'].apply(len)\n",
        "\n",
        "# ---- 4. Identify Common Words (Word Frequency) ----\n",
        "# Flatten all words into a single list\n",
        "all_words = \" \".join(df_subset['clean_text']).split()\n",
        "word_counts = Counter(all_words)\n",
        "\n",
        "# Get the 20 most common words\n",
        "common_words = word_counts.most_common(20)\n",
        "\n",
        "# Convert to DataFrame for visualization\n",
        "df_common_words = pd.DataFrame(common_words, columns=[\"Word\", \"Count\"])\n",
        "\n",
        "# ---- 5. Hashtag Analysis ----\n",
        "df_subset['hashtags'] = df_subset['clean_text'].apply(lambda x: re.findall(r'#\\w+', x))\n",
        "all_hashtags = [tag.lower() for sublist in df_subset['hashtags'] for tag in sublist]\n",
        "hashtag_counts = Counter(all_hashtags).most_common(10)\n",
        "df_hashtags = pd.DataFrame(hashtag_counts, columns=[\"Hashtag\", \"Count\"])\n",
        "\n",
        "# ---- 6. Mentions Analysis (@username) ----\n",
        "df_subset['mentions'] = df_subset['clean_text'].apply(lambda x: re.findall(r'@\\w+', x))\n",
        "all_mentions = [mention.lower() for sublist in df_subset['mentions'] for mention in sublist]\n",
        "mention_counts = Counter(all_mentions).most_common(10)\n",
        "df_mentions = pd.DataFrame(mention_counts, columns=[\"Mention\", \"Count\"])\n",
        "\n",
        "# ---- 7. Create Subplots for EDA ----\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 12))  # 2 rows, 2 columns\n",
        "fig.suptitle(\"Exploratory Data Analysis (EDA)\", fontsize=16, fontweight='bold')  # Big title\n",
        "\n",
        "# Plot 1: Distribution of Tweet Lengths\n",
        "sns.histplot(df_subset['text_length'], bins=30, kde=True, ax=axes[0, 0])\n",
        "axes[0, 0].set_title(\"Distribution of Tweet Lengths\", fontsize=12)\n",
        "axes[0, 0].set_xlabel(\"Tweet Length (Characters)\")\n",
        "axes[0, 0].set_ylabel(\"Frequency\")\n",
        "\n",
        "# Plot 2: Word Cloud of Most Common Words --> the most commom words are the largest in size in the cloud.\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(\" \".join(all_words))\n",
        "axes[0, 1].imshow(wordcloud, interpolation='bilinear')\n",
        "axes[0, 1].set_title(\"Word Cloud of Common Words\", fontsize=12)\n",
        "axes[0, 1].axis(\"off\")\n",
        "\n",
        "# Plot 3: Hashtag Analysis\n",
        "sns.barplot(x=\"Count\", y=\"Hashtag\", data=df_hashtags, ax=axes[1, 0])\n",
        "axes[1, 0].set_title(\"Top 10 Most Used Hashtags\", fontsize=12)\n",
        "axes[1, 0].set_xlabel(\"Count\")\n",
        "axes[1, 0].set_ylabel(\"Hashtag\")\n",
        "\n",
        "# Plot 4: Mentions Analysis (@username)\n",
        "sns.barplot(x=\"Count\", y=\"Mention\", data=df_mentions, ax=axes[1, 1])\n",
        "axes[1, 1].set_title(\"Top 10 Most Mentioned Users\", fontsize=12)\n",
        "axes[1, 1].set_xlabel(\"Count\")\n",
        "axes[1, 1].set_ylabel(\"Mention\")\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.95], h_pad=4)  # Leaves space for big title and adds vertical padding\n",
        "plt.show()\n",
        "\n",
        "# ---- 8. Display Insights ----\n",
        "print(\"Tweet Length Summary Statistics:\")\n",
        "print(df_subset['text_length'].describe())\n",
        "\n",
        "print(\"\\nMost Common Words:\")\n",
        "print(df_common_words)\n",
        "\n",
        "print(\"\\nMost Used Hashtags:\")\n",
        "print(df_hashtags)\n",
        "\n",
        "print(\"\\nMost Mentioned Users:\")\n",
        "print(df_mentions)\n",
        "\n",
        "print(\"\\n################################################################################################\\n\")"
      ],
      "metadata": {
        "id": "smOx6k_z6wVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 4. Feature Transformation & Topic Modeling**"
      ],
      "metadata": {
        "id": "0EDWA4om7qQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Feature Transformation & Topic Modeling\n",
        "# --------------------------------------------------\n",
        "# In this step, we convert text data into numerical features and apply topic modeling.\n",
        "# We will:\n",
        "# 1. Transform text using TF-IDF (Term Frequency-Inverse Document Frequency)\n",
        "# 2. Apply clustering algorithms: K-Means, DBSCAN\n",
        "# 3. Use LDA (Latent Dirichlet Allocation) for topic extraction\n",
        "# --------------------------------------------------\n",
        "\n",
        "# ---- 1. Install Necessary Libraries ----\n",
        "try:\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer  # Convert text to numerical features\n",
        "    from sklearn.cluster import KMeans, DBSCAN  # Clustering algorithms\n",
        "    from sklearn.decomposition import LatentDirichletAllocation  # Topic modeling\n",
        "    import numpy as np  # Numerical operations\n",
        "    from textblob import TextBlob # Sentiment analysis\n",
        "except ImportError:\n",
        "    import os\n",
        "    os.system('pip install scikit-learn numpy textblob')\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    from sklearn.cluster import KMeans, DBSCAN\n",
        "    from sklearn.decomposition import LatentDirichletAllocation\n",
        "    import numpy as np\n",
        "    from textblob import TextBlob\n",
        "\n",
        "print(\"Step 4: Feature Transformation & Topic Modeling\\n\")\n",
        "\n",
        "# ---- 3. Transform Text Using TF-IDF ----\n",
        "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
        "X_tfidf = vectorizer.fit_transform(df_subset['clean_text'])\n",
        "\n",
        "# Convert to DataFrame for better readability\n",
        "df_tfidf = pd.DataFrame(X_tfidf.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "print(\"TF-IDF Transformation Completed. Shape:\", df_tfidf.shape, \"\\n\")\n",
        "\n",
        "# ---- 4. Apply K-Means Clustering ----\n",
        "n_clusters = 5  # Define number of clusters\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "df_subset['cluster_kmeans'] = kmeans.fit_predict(X_tfidf)\n",
        "print(\"K-Means Clustering Applied. Number of Clusters:\", n_clusters, \"\\n\")\n",
        "\n",
        "# ---- 5. Apply DBSCAN Clustering ----\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "df_subset['cluster_dbscan'] = dbscan.fit_predict(X_tfidf)\n",
        "print(\"DBSCAN Clustering Applied.\\n\")\n",
        "\n",
        "# ---- 6. Apply LDA for Topic Modeling ----\n",
        "n_topics = 5  # Define number of topics\n",
        "lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
        "lda.fit(X_tfidf)\n",
        "\n",
        "# Extract topics\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "for i, topic in enumerate(lda.components_):\n",
        "    print(f\"Topic {i+1}:\", [terms[i] for i in topic.argsort()[-10:]])\n",
        "\n",
        "# ---- 7. Visualizing Cluster Distributions ----\n",
        "# Create a column for the dominant LDA topic of each tweet\n",
        "doc_topics = lda.transform(X_tfidf)\n",
        "df_subset['dominant_topic'] = doc_topics.argmax(axis=1)\n",
        "\n",
        "# ---- 7. Create a Single Window with 3 Subplots ----\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))  # 1 row, 3 columns for subplots\n",
        "\n",
        "# Add a big title for the entire figure\n",
        "fig.suptitle(\"Comparison of K-Means, DBSCAN, and LDA Distributions\", fontsize=16)\n",
        "\n",
        "# ---- Subplot 1: K-Means Distribution ----\n",
        "sns.countplot(ax=axes[0], x='cluster_kmeans', data=df_subset)\n",
        "axes[0].set_title(\"K-Means Cluster Distribution\")\n",
        "axes[0].set_xlabel(\"K-Means Clusters\")\n",
        "axes[0].set_ylabel(\"Tweet Count\")\n",
        "\n",
        "# ---- Subplot 2: DBSCAN Distribution ----\n",
        "sns.countplot(ax=axes[1], x='cluster_dbscan', data=df_subset)\n",
        "axes[1].set_title(\"DBSCAN Cluster Distribution\")\n",
        "axes[1].set_xlabel(\"DBSCAN Clusters\")\n",
        "axes[1].set_ylabel(\"Tweet Count\")\n",
        "\n",
        "# ---- Subplot 3: LDA Topic Distribution ----\n",
        "sns.countplot(ax=axes[2], x='dominant_topic', data=df_subset)\n",
        "axes[2].set_title(\"LDA Dominant Topic Distribution\")\n",
        "axes[2].set_xlabel(\"Dominant Topic\")\n",
        "axes[2].set_ylabel(\"Tweet Count\")\n",
        "\n",
        "# Adjust layout to avoid overlap\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n################################################################################################\\n\")"
      ],
      "metadata": {
        "id": "YE0-wgXB6vva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 5. BERT-Based Feature Transformation & Topic Modeling**"
      ],
      "metadata": {
        "id": "hffQddXL70Wl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: BERT-Based Feature Transformation & Topic Modeling\n",
        "# --------------------------------------------------\n",
        "# In this step, we leverage BERT embeddings for text representation and apply clustering.\n",
        "# We will:\n",
        "# 1. Convert text into BERT embeddings (768-dimensional vector representation)\n",
        "# 2. Apply K-Means clustering to group similar tweets\n",
        "# 3. Visualize the clusters using PCA for dimensionality reduction\n",
        "# --------------------------------------------------\n",
        "\n",
        "# ---- 1. Install Necessary Libraries ----\n",
        "try:\n",
        "    from transformers import BertTokenizer, BertModel  # Pre-trained BERT model & tokenizer\n",
        "    import torch  # Deep learning framework for BERT\n",
        "    from sklearn.decomposition import PCA  # Dimensionality reduction for visualization\n",
        "except ImportError:\n",
        "    import os\n",
        "    os.system(\"pip install transformers torch scikit-learn numpy pandas matplotlib\")\n",
        "    from transformers import BertTokenizer, BertModel\n",
        "    import torch\n",
        "    from sklearn.decomposition import PCA\n",
        "\n",
        "# ---- 2. Load Pre-trained BERT Model & Tokenizer ----\n",
        "print(\"Step 5: BERT-Based Feature Transformation & Topic Modeling\\n\")\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# ---- 3. Convert Text into BERT Embeddings ----\n",
        "def get_bert_embedding(text):\n",
        "    \"\"\"Convert text into a 768-dimensional BERT embedding.\"\"\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "\n",
        "    with torch.no_grad():  # No gradients needed for inference\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    return outputs.last_hidden_state[:, 0, :].numpy().flatten()  # Extract sentence embedding\n",
        "\n",
        "df_subset[\"bert_embedding\"] = df_subset[\"clean_text\"].apply(get_bert_embedding)\n",
        "\n",
        "# Convert embeddings to NumPy array\n",
        "X_bert = np.vstack(df_subset[\"bert_embedding\"].values)\n",
        "print(\"BERT Embeddings Generated. Shape:\", X_bert.shape, \"\\n\")  # Should be (num_samples, 768)\n",
        "\n",
        "# ---- 4. Apply K-Means Clustering ----\n",
        "n_clusters = 5  # Define number of clusters\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "df_subset[\"bert_cluster\"] = kmeans.fit_predict(X_bert)\n",
        "\n",
        "print(\"K-Means Clustering Applied. Number of Clusters:\", n_clusters, \"\\n\")\n",
        "\n",
        "# ---- 5. Visualize Clusters Using PCA ----\n",
        "pca = PCA(n_components=2)  # Reduce dimensions from 768 to 2 for visualization\n",
        "X_pca = pca.fit_transform(X_bert)\n",
        "\n",
        "# Scatter plot of clusters\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=df_subset[\"bert_cluster\"], cmap=\"viridis\", alpha=0.6)\n",
        "plt.title(\"BERT-Based Tweet Clusters\")\n",
        "plt.xlabel(\"PCA Dimension 1\")\n",
        "plt.ylabel(\"PCA Dimension 2\")\n",
        "plt.colorbar(label=\"Cluster\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n################################################################################################\\n\")"
      ],
      "metadata": {
        "id": "e3cj3o7G6vcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 6. Focus on iPhone Brand and Apply Topic Modeling & Clustering**"
      ],
      "metadata": {
        "id": "bCGVgXKZ8D2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Focus on iPhone Brand and Apply Topic Modeling & Clustering\n",
        "# --------------------------------------------------\n",
        "# In this step, we will:\n",
        "# 1. Filter tweets mentioning the iPhone brand (or related hashtags).\n",
        "# 2. Apply TF-IDF transformation to the filtered tweets.\n",
        "# 3. Apply clustering algorithms (K-Means, DBSCAN).\n",
        "# 4. Perform topic modeling with LDA to extract topics related to iPhone.\n",
        "# 5. Optionally, apply sentiment analysis to the iPhone-related tweets.\n",
        "\n",
        "# ---- 1. Filter Tweets Mentioning iPhone ----\n",
        "iphone_keywords = ['#iPhone', 'iPhone', 'iphone', 'iPhone6', 'iPhoneX', 'iPhone13', 'iPhone12', 'iPhone14', '#Apple']\n",
        "# Filter the dataset for tweets containing \"iPhone\" or related hashtags\n",
        "iphone_df = df_subset[df_subset['clean_text'].str.contains('|'.join(iphone_keywords), case=False, na=False)]\n",
        "\n",
        "# Display the number of iPhone-related tweets\n",
        "print(f\"Number of iPhone-related tweets: {iphone_df.shape[0]}\\n\")\n",
        "\n",
        "# ---- 2. Apply TF-IDF Transformation ----\n",
        "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
        "X_tfidf_iphone = vectorizer.fit_transform(iphone_df['clean_text'])\n",
        "\n",
        "# Convert TF-IDF to a DataFrame for better readability\n",
        "df_tfidf_iphone = pd.DataFrame(X_tfidf_iphone.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "print(\"TF-IDF Transformation Completed for iPhone-related tweets. Shape:\", df_tfidf_iphone.shape, \"\\n\")\n",
        "\n",
        "# ---- 3. Apply K-Means Clustering to iPhone Tweets ----\n",
        "n_clusters_iphone = 5  # Define number of clusters\n",
        "kmeans_iphone = KMeans(n_clusters=n_clusters_iphone, random_state=42, n_init=10)\n",
        "iphone_df['cluster_kmeans'] = kmeans_iphone.fit_predict(X_tfidf_iphone)\n",
        "print(\"K-Means Clustering Applied on iPhone tweets. Number of Clusters:\", n_clusters_iphone, \"\\n\")\n",
        "\n",
        "# ---- 4. Apply DBSCAN Clustering to iPhone Tweets ----\n",
        "dbscan_iphone = DBSCAN(eps=0.5, min_samples=5)\n",
        "iphone_df['cluster_dbscan'] = dbscan_iphone.fit_predict(X_tfidf_iphone)\n",
        "print(\"DBSCAN Clustering Applied on iPhone tweets.\\n\")\n",
        "\n",
        "# ---- 5. Apply LDA for Topic Modeling on iPhone Tweets ----\n",
        "n_topics_iphone = 5  # Define number of topics for iPhone-related tweets\n",
        "lda_iphone = LatentDirichletAllocation(n_components=n_topics_iphone, random_state=42)\n",
        "lda_iphone.fit(X_tfidf_iphone)\n",
        "\n",
        "# Extract and display topics for iPhone-related tweets\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "for i, topic in enumerate(lda_iphone.components_):\n",
        "    print(f\"Topic {i+1} (iPhone-related):\", [terms[i] for i in topic.argsort()[-10:]])\n",
        "\n",
        "# ---- 6. Apply Sentiment Analysis on iPhone Tweets ----\n",
        "\n",
        "# Function to analyze sentiment\n",
        "def get_sentiment(text):\n",
        "    blob = TextBlob(text)\n",
        "    # Polarity ranges from -1 (negative) to 1 (positive)\n",
        "    return blob.sentiment.polarity\n",
        "\n",
        "# Apply sentiment analysis to the iPhone-related tweets\n",
        "iphone_df['sentiment'] = iphone_df['clean_text'].apply(get_sentiment)\n",
        "\n",
        "# Display sentiment distribution for iPhone-related tweets\n",
        "print(\"\\nSentiment distribution for iPhone-related tweets:\")\n",
        "print(iphone_df['sentiment'].describe())\n",
        "\n",
        "# ---- 7. Visualizing K-Means Clusters ----\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Visualize K-Means clusters for iPhone-related tweets\n",
        "sns.countplot(x='cluster_kmeans', data=iphone_df)\n",
        "plt.title(\"Cluster Distribution of iPhone-related Tweets (K-Means)\")\n",
        "plt.xlabel(\"Cluster Number\")\n",
        "plt.ylabel(\"Tweet Count\")\n",
        "plt.show()\n",
        "\n",
        "# ---- 8. Visualizing Sentiment Distribution for iPhone Tweets ----\n",
        "sns.histplot(iphone_df['sentiment'], kde=True)\n",
        "plt.title(\"Sentiment Distribution of iPhone-related Tweets\")\n",
        "plt.xlabel(\"Sentiment Score\")\n",
        "plt.ylabel(\"Tweet Count\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n################################################################################################\\n\")"
      ],
      "metadata": {
        "id": "Iwp_h0U063dL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}